# üöó License Plate Recognition System - YOLOv8 + EasyOCR

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![YOLO](https://img.shields.io/badge/YOLO-v8-red.svg)](https://github.com/ultralytics/ultralytics)
[![EasyOCR](https://img.shields.io/badge/EasyOCR-1.6+-green.svg)](https://github.com/JaidedAI/EasyOCR)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.5+-orange.svg)](https://opencv.org/)

## üìã T·ªïng quan d·ª± √°n

H·ªá th·ªëng nh·∫≠n di·ªán bi·ªÉn s·ªë xe s·ª≠ d·ª•ng **YOLOv8** ƒë·ªÉ ph√°t hi·ªán v·ªã tr√≠ bi·ªÉn s·ªë v√† **EasyOCR** ƒë·ªÉ ƒë·ªçc vƒÉn b·∫£n. D·ª± √°n bao g·ªìm quy tr√¨nh ho√†n ch·ªânh t·ª´ vi·ªác thu th·∫≠p d·ªØ li·ªáu, train model YOLO, ƒë·∫øn vi·ªác tri·ªÉn khai h·ªá th·ªëng nh·∫≠n di·ªán v·ªõi giao di·ªán GUI.

## üéØ T√≠nh nƒÉng ch√≠nh

- **üîç Ph√°t hi·ªán bi·ªÉn s·ªë**: S·ª≠ d·ª•ng YOLOv8 model ƒë√£ ƒë∆∞·ª£c train custom
- **üìñ OCR vƒÉn b·∫£n**: ƒê·ªçc bi·ªÉn s·ªë b·∫±ng EasyOCR v·ªõi ƒë·ªô ch√≠nh x√°c cao
- **üñ•Ô∏è Giao di·ªán GUI**: Tkinter interface th√¢n thi·ªán v√† d·ªÖ s·ª≠ d·ª•ng
- **üñºÔ∏è X·ª≠ l√Ω ·∫£nh**: Pipeline tƒÉng c∆∞·ªùng ch·∫•t l∆∞·ª£ng ·∫£nh t·ª± ƒë·ªông
- **üíæ L∆∞u tr·ªØ k·∫øt qu·∫£**: Export ·∫£nh v·ªõi bounding box v√† text ƒë∆∞·ª£c nh·∫≠n di·ªán
- **‚ö° Multi-threading**: X·ª≠ l√Ω kh√¥ng block giao di·ªán ng∆∞·ªùi d√πng

## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Input Image   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   YOLOv8 Model  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  License Plate  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ   (Detection)   ‚îÇ    ‚îÇ   Detection     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ  Image Preproc  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   EasyOCR       ‚îÇ
                       ‚îÇ  (Enhancement)  ‚îÇ    ‚îÇ   (Text Read)   ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                       ‚îÇ
                                ‚ñº                       ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ  Result Image   ‚îÇ    ‚îÇ  Detected Text  ‚îÇ
                       ‚îÇ  (BBox + Text)  ‚îÇ    ‚îÇ  (License #)    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ C·∫•u tr√∫c d·ª± √°n

```
license-plate-recognition/
‚îú‚îÄ‚îÄ üìÅ YOLO/                          # Th∆∞ m·ª•c ch√≠nh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.py                    # Script command line
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main_gui.py                # Giao di·ªán GUI
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ requirements.txt            # Dependencies
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README.md                  # H∆∞·ªõng d·∫´n n√†y
‚îÇ   ‚îú‚îÄ‚îÄ üéØ best.pt                    # Model YOLO ƒë√£ train
‚îÇ   ‚îú‚îÄ‚îÄ üéØ yolov8n.pt                 # Model YOLO g·ªëc
‚îÇ   ‚îú‚îÄ‚îÄ üñºÔ∏è bien1.jpg                 # ·∫¢nh test 1
‚îÇ   ‚îú‚îÄ‚îÄ üñºÔ∏è bien2.jpg                 # ·∫¢nh test 2
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ coco128/                   # Dataset training
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ images/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ train2017/         # ·∫¢nh training
‚îÇ       ‚îî‚îÄ‚îÄ üìÅ labels/
‚îÇ           ‚îî‚îÄ‚îÄ üìÅ train2017/         # Labels YOLO format
‚îú‚îÄ‚îÄ üìÅ dataset/                       # Dataset g·ªëc
‚îú‚îÄ‚îÄ üìÅ training/                      # Scripts training
‚îî‚îÄ‚îÄ üìÅ docs/                          # T√†i li·ªáu
```

## üõ†Ô∏è C√†i ƒë·∫∑t v√† thi·∫øt l·∫≠p

### 1. Y√™u c·∫ßu h·ªá th·ªëng

- **Python**: 3.8+ 
- **RAM**: T·ªëi thi·ªÉu 8GB (16GB+ khuy·∫øn ngh·ªã)
- **GPU**: NVIDIA GPU v·ªõi CUDA (khuy·∫øn ngh·ªã)
- **OS**: Windows 10/11, Linux, macOS

### 2. Clone repository

```bash
git clone https://github.com/yourusername/license-plate-recognition.git
cd license-plate-recognition/YOLO
```

### 3. C√†i ƒë·∫∑t dependencies

```bash
# C√†i ƒë·∫∑t PyTorch (ch·ªçn version ph√π h·ª£p v·ªõi CUDA)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán kh√°c
pip install -r requirements.txt
```

### 4. Ki·ªÉm tra c√†i ƒë·∫∑t

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import ultralytics; print(f'Ultralytics: {ultralytics.__version__}')"
python -c "import easyocr; print('EasyOCR: OK')"
```

## üéì H∆∞·ªõng d·∫´n Training YOLO Model

### B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu

#### 1.1 Thu th·∫≠p ·∫£nh bi·ªÉn s·ªë xe
- **S·ªë l∆∞·ª£ng**: T·ªëi thi·ªÉu 500-1000 ·∫£nh
- **ƒêa d·∫°ng**: G√≥c ch·ª•p, √°nh s√°ng, ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt kh√°c nhau
- **Ch·∫•t l∆∞·ª£ng**: ƒê·ªô ph√¢n gi·∫£i cao, r√µ n√©t

#### 1.2 Chu·∫©n b·ªã dataset theo format YOLO

```bash
dataset/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ train/          # 80% ·∫£nh training
‚îÇ   ‚îú‚îÄ‚îÄ val/            # 20% ·∫£nh validation
‚îÇ   ‚îî‚îÄ‚îÄ test/           # ·∫¢nh test (optional)
‚îî‚îÄ‚îÄ labels/
    ‚îú‚îÄ‚îÄ train/          # Labels t∆∞∆°ng ·ª©ng
    ‚îú‚îÄ‚îÄ val/
    ‚îî‚îÄ‚îÄ test/
```

#### 1.3 Format label YOLO
M·ªói file `.txt` ch·ª©a:
```
class_id center_x center_y width height
```
V√≠ d·ª•:
```
0 0.5 0.5 0.3 0.2
```

### B∆∞·ªõc 2: C·∫•u h√¨nh training

#### 2.1 T·∫°o file `data.yaml`

```yaml
# data.yaml
path: ../dataset  # ƒê∆∞·ªùng d·∫´n t·ªõi dataset
train: images/train  # ·∫¢nh training
val: images/val      # ·∫¢nh validation

# S·ªë classes
nc: 1

# T√™n classes
names:
  0: license_plate
```

#### 2.2 T·∫°o file `model.yaml` (t√πy ch·ªçn)

```yaml
# model.yaml
nc: 1  # S·ªë classes
depth_multiple: 1.0  # Depth scaling
width_multiple: 1.0  # Width scaling
```

### B∆∞·ªõc 3: Training model

#### 3.1 Training t·ª´ ƒë·∫ßu

```python
from ultralytics import YOLO

# T·∫°o model m·ªõi
model = YOLO('yolov8n.yaml')  # yolov8n, yolov8s, yolov8m, yolov8l, yolov8x

# Training
results = model.train(
    data='data.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    name='license_plate_detection'
)
```

#### 3.2 Transfer learning

```python
# Load pre-trained model
model = YOLO('yolov8n.pt')

# Fine-tuning
results = model.train(
    data='data.yaml',
    epochs=50,
    imgsz=640,
    batch=16,
    pretrained=True,
    name='license_plate_finetune'
)
```

#### 3.3 Training v·ªõi custom parameters

```python
results = model.train(
    data='data.yaml',
    epochs=200,
    imgsz=640,
    batch=8,
    lr0=0.01,
    lrf=0.1,
    momentum=0.937,
    weight_decay=0.0005,
    warmup_epochs=3,
    warmup_momentum=0.8,
    warmup_bias_lr=0.1,
    box=7.5,
    cls=0.5,
    dfl=1.5,
    pose=12.0,
    kobj=1.0,
    label_smoothing=0.0,
    nbs=64,
    overlap_mask=True,
    mask_ratio=4,
    dropout=0.0,
    val=True,
    plots=True,
    save=True,
    save_period=10
)
```

### B∆∞·ªõc 4: ƒê√°nh gi√° v√† t·ªëi ∆∞u

#### 4.1 Ki·ªÉm tra metrics

```python
# Validate model
metrics = model.val()

# Xem k·∫øt qu·∫£
print(f"mAP50: {metrics.box.map50}")
print(f"mAP50-95: {metrics.box.map}")
print(f"Precision: {metrics.box.p}")
print(f"Recall: {metrics.box.r}")
```

#### 4.2 Export model

```python
# Export sang c√°c format kh√°c
model.export(format='onnx')      # ONNX
model.export(format='tflite')    # TensorFlow Lite
model.export(format='coreml')    # Core ML
```

## üîç S·ª≠ d·ª•ng EasyOCR

### 1. Kh·ªüi t·∫°o EasyOCR

```python
import easyocr

# Kh·ªüi t·∫°o reader v·ªõi ng√¥n ng·ªØ ti·∫øng Anh
reader = easyocr.Reader(['en'])

# H·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ
reader = easyocr.Reader(['en', 'vi'])  # Ti·∫øng Anh + Ti·∫øng Vi·ªát
```

### 2. ƒê·ªçc vƒÉn b·∫£n t·ª´ ·∫£nh

#### 2.1 ƒê·ªçc ƒë∆°n gi·∫£n

```python
# ƒê·ªçc t·ª´ ·∫£nh
results = reader.readtext('image.jpg')

# K·∫øt qu·∫£: [(bbox, text, confidence), ...]
for (bbox, text, prob) in results:
    print(f'Text: {text}, Confidence: {prob:.2f}')
```

#### 2.2 ƒê·ªçc v·ªõi preprocessing

```python
import cv2
import numpy as np

def enhance_image(image):
    """TƒÉng c∆∞·ªùng ch·∫•t l∆∞·ª£ng ·∫£nh"""
    # TƒÉng k√≠ch th∆∞·ªõc
    scale_factor = 3
    height, width = image.shape[:2]
    new_width = width * scale_factor
    new_height = height * scale_factor
    
    resized = cv2.resize(image, (new_width, new_height), 
                        interpolation=cv2.INTER_CUBIC)
    
    # L√†m m·ªãn
    smoothed = cv2.GaussianBlur(resized, (3, 3), 0)
    
    # TƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n
    lab = cv2.cvtColor(smoothed, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    cl = clahe.apply(l)
    enhanced = cv2.merge((cl,a,b))
    
    return cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)

# √Åp d·ª•ng preprocessing
enhanced_image = enhance_image(cropped_plate)
results = reader.readtext(enhanced_image)
```

### 3. T·ªëi ∆∞u h√≥a OCR

#### 3.1 L·ªçc k·∫øt qu·∫£ theo confidence

```python
# L·ªçc theo ƒë·ªô tin c·∫≠y
filtered_results = []
for (bbox, text, prob) in results:
    if prob > 0.5:  # Ch·ªâ l·∫•y k·∫øt qu·∫£ c√≥ confidence > 50%
        filtered_results.append((bbox, text, prob))
```

#### 3.2 S·∫Øp x·∫øp theo v·ªã tr√≠

```python
# S·∫Øp x·∫øp theo v·ªã tr√≠ Y (t·ª´ tr√™n xu·ªëng d∆∞·ªõi)
text_with_positions = []
for (bbox, text, prob) in results:
    if prob > 0.2:
        y_pos = np.mean([point[1] for point in bbox])
        text_with_positions.append((text, prob, y_pos))

text_with_positions.sort(key=lambda x: x[2])

# K·∫øt h·ª£p text th√†nh bi·ªÉn s·ªë ho√†n ch·ªânh
if len(text_with_positions) >= 2:
    plate_text = f"{text_with_positions[0][0]} {text_with_positions[1][0]}"
elif len(text_with_positions) == 1:
    plate_text = text_with_positions[0][0]
else:
    plate_text = "Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c"
```

## üöÄ S·ª≠ d·ª•ng h·ªá th·ªëng

### 1. Ch·∫°y GUI (Khuy·∫øn ngh·ªã)

```bash
python main_gui.py
```

**H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng GUI:**
1. **üìÅ T·∫£i ·∫¢nh**: Ch·ªçn ·∫£nh bi·ªÉn s·ªë xe c·∫ßn nh·∫≠n di·ªán
2. **üîç Nh·∫≠n Di·ªán**: B·∫Øt ƒë·∫ßu qu√° tr√¨nh ph√°t hi·ªán v√† OCR
3. **üéØ Xem K·∫øt Qu·∫£**: K·∫øt qu·∫£ hi·ªÉn th·ªã v·ªõi bounding box v√† text
4. **üíæ L∆∞u K·∫øt Qu·∫£**: L∆∞u ·∫£nh k·∫øt qu·∫£

### 2. Ch·∫°y Command Line

```bash
# S·ª≠ d·ª•ng ·∫£nh m·∫∑c ƒë·ªãnh
python main.py

# T√πy ch·ªânh ·∫£nh input
python -c "
from ultralytics import YOLO
import cv2
model = YOLO('best.pt')
results = model('your_image.jpg')
"
```

### 3. S·ª≠ d·ª•ng trong code

```python
from ultralytics import YOLO
import easyocr
import cv2

# Load models
yolo_model = YOLO('best.pt')
ocr_reader = easyocr.Reader(['en'])

# Nh·∫≠n di·ªán bi·ªÉn s·ªë
image = cv2.imread('license_plate.jpg')
results = yolo_model(image)

# X·ª≠ l√Ω k·∫øt qu·∫£
for result in results:
    boxes = result.boxes.xyxy
    for box in boxes:
        x1, y1, x2, y2 = map(int, box)
        
        # Crop bi·ªÉn s·ªë
        cropped = image[y1:y2, x1:x2]
        
        # OCR
        text_results = ocr_reader.readtext(cropped)
        if text_results:
            text = text_results[0][1]
            confidence = text_results[0][2]
            print(f"Bi·ªÉn s·ªë: {text}, Confidence: {confidence:.2f}")
```

## üìä K·∫øt qu·∫£ v√† ƒë√°nh gi√°

### 1. Metrics ƒë√°nh gi√°

- **mAP50**: Mean Average Precision at IoU=0.5
- **mAP50-95**: Mean Average Precision at IoU=0.5:0.95
- **Precision**: ƒê·ªô ch√≠nh x√°c
- **Recall**: ƒê·ªô bao ph·ªß
- **F1-Score**: Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall

### 2. K·∫øt qu·∫£ m·∫´u

| Model | mAP50 | mAP50-95 | Precision | Recall | F1-Score |
|-------|-------|----------|-----------|---------|----------|
| YOLOv8n | 0.89 | 0.67 | 0.91 | 0.87 | 0.89 |
| YOLOv8s | 0.92 | 0.71 | 0.93 | 0.90 | 0.92 |
| YOLOv8m | 0.94 | 0.75 | 0.95 | 0.93 | 0.94 |

### 3. So s√°nh v·ªõi baseline

- **YOLOv5**: T·ªëc ƒë·ªô nhanh, ƒë·ªô ch√≠nh x√°c th·∫•p h∆°n
- **YOLOv7**: C√¢n b·∫±ng t·ªët gi·ªØa t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c
- **YOLOv8**: ƒê·ªô ch√≠nh x√°c cao nh·∫•t, t·ªëc ƒë·ªô ch·∫•p nh·∫≠n ƒë∆∞·ª£c

## üîß T·ªëi ∆∞u h√≥a v√† tuning

### 1. Hyperparameter tuning

```python
# Grid search cho learning rate
lr_values = [0.001, 0.01, 0.1]
for lr in lr_values:
    model.train(
        data='data.yaml',
        epochs=50,
        lr0=lr,
        name=f'experiment_lr_{lr}'
    )
```

### 2. Data augmentation

```python
# TƒÉng c∆∞·ªùng d·ªØ li·ªáu
model.train(
    data='data.yaml',
    epochs=100,
    augment=True,
    degrees=10.0,      # Xoay ·∫£nh
    translate=0.1,     # D·ªãch chuy·ªÉn
    scale=0.5,         # Thay ƒë·ªïi t·ª∑ l·ªá
    shear=2.0,         # Bi·∫øn d·∫°ng
    perspective=0.0,    # G√≥c nh√¨n
    flipud=0.0,        # L·∫≠t d·ªçc
    fliplr=0.5,        # L·∫≠t ngang
    mosaic=1.0,        # Mosaic augmentation
    mixup=0.0          # Mixup augmentation
)
```

### 3. Model pruning v√† quantization

```python
# Pruning model
model.prune()

# Quantization
model.export(format='tflite', int8=True)
```

## üö® Troubleshooting

### 1. L·ªói th∆∞·ªùng g·∫∑p

#### Model kh√¥ng load ƒë∆∞·ª£c
```bash
# Ki·ªÉm tra CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Ki·ªÉm tra model file
ls -la *.pt
```

#### OCR kh√¥ng ƒë·ªçc ƒë∆∞·ª£c text
```python
# Gi·∫£m threshold
results = reader.readtext(image, confidence_threshold=0.1)

# Th·ª≠ preprocessing kh√°c
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
results = reader.readtext(gray)
```

#### Memory issues
```python
# Gi·∫£m batch size
model.train(batch=4)

# S·ª≠ d·ª•ng mixed precision
model.train(amp=True)
```

### 2. Performance tips

- **GPU**: S·ª≠ d·ª•ng NVIDIA GPU v·ªõi CUDA
- **Batch size**: ƒêi·ªÅu ch·ªânh theo RAM kh·∫£ d·ª•ng
- **Image size**: Gi·∫£m `imgsz` n·∫øu c·∫ßn
- **Model size**: Ch·ªçn model ph√π h·ª£p (n < s < m < l < x)

## üìà C·∫£i ti·∫øn v√† m·ªü r·ªông

### 1. T√≠nh nƒÉng m·ªõi

- [ ] **Video processing**: X·ª≠ l√Ω video real-time
- [ ] **Multi-language**: H·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ
- [ ] **Database**: L∆∞u tr·ªØ k·∫øt qu·∫£ v√†o database
- [ ] **API**: REST API cho web/mobile
- [ ] **Cloud**: Deploy l√™n cloud platform

### 2. Model improvements

- [ ] **Ensemble**: K·∫øt h·ª£p nhi·ªÅu model
- [ ] **Attention**: Th√™m attention mechanism
- [ ] **Transformer**: S·ª≠ d·ª•ng Vision Transformer
- [ ] **AutoML**: T·ª± ƒë·ªông t√¨m hyperparameters t·ªëi ∆∞u

### 3. Deployment

- [ ] **Docker**: Containerization
- [ ] **Kubernetes**: Orchestration
- [ ] **Edge**: Deploy tr√™n edge devices
- [ ] **Mobile**: T·ªëi ∆∞u cho mobile

## ü§ù ƒê√≥ng g√≥p

Ch√∫ng t√¥i r·∫•t hoan ngh√™nh m·ªçi ƒë√≥ng g√≥p! H√£y:

1. Fork repository
2. T·∫°o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. M·ªü Pull Request

## üìö T√†i li·ªáu tham kh·∫£o

- [YOLOv8 Documentation](https://docs.ultralytics.com/)
- [EasyOCR Documentation](https://github.com/JaidedAI/EasyOCR)
- [OpenCV Documentation](https://docs.opencv.org/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Computer Vision Papers](https://paperswithcode.com/)

## üìÑ License

D·ª± √°n n√†y ƒë∆∞·ª£c ph√¢n ph·ªëi d∆∞·ªõi gi·∫•y ph√©p MIT. Xem file `LICENSE` ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt.

## üë• T√°c gi·∫£

- **V√µ ƒê·ª©c H√≤a** - [@voduchoa](https://github.com/voduchoa)
- **Email**: voduchoa@example.com
- **LinkedIn**: [V√µ ƒê·ª©c H√≤a](https://linkedin.com/in/voduchoa)

## üôè C·∫£m ∆°n

- [Ultralytics](https://github.com/ultralytics/ultralytics) cho YOLOv8
- [EasyOCR](https://github.com/JaidedAI/EasyOCR) cho OCR engine
- [OpenCV](https://opencv.org/) cho computer vision
- [PyTorch](https://pytorch.org/) cho deep learning framework

---

‚≠ê **N·∫øu d·ª± √°n n√†y h·ªØu √≠ch, h√£y cho ch√∫ng t√¥i m·ªôt star!** ‚≠ê
